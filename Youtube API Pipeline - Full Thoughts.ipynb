{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Data Pipeline in Python\n",
    "\n",
    "The goal of this project is to load in data from a YouTube channel API and extract useful data in a dataframe format, then upload that to an AWS database. This version contains my entire thought process, for a version with only code see: https://github.com/Andrew-Castagno/Portfolio/blob/main/Youtube%20API%20NLP%20Pipeline%20-%20Code%20Only.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg2 as ps \n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure():\n",
    "    load_dotenv() #securely loading in my credentials from .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I will be looking at the popular science channel, Kurzgesagt. In order to find the channel ID, we obtain it from the source code on the YouTube channel's homepage. We also need the base url from which we will form the root of our api, this can be found in the documentation: https://developers.google.com/youtube/v3/docs/search/list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key and ID, you will want to replace the API key with your own\n",
    "configure()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "CHANNEL_ID = \"UCsXVk37bltHxD1rDPwtNM8Q\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Exploration\n",
    "\n",
    "First, I will craft an API from the base URL and the parameters found in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://www.googleapis.com/youtube/v3/search?key={API_KEY}&channelId={CHANNEL_ID}&part=snippet,id&order=date&maxResults=2000\"\n",
    "\n",
    "video_info = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a few options to pick from for our statistics, including like count, view count, comment count, and favorite count. Favorite count is always zero, so we will leave it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df = pd.DataFrame(columns = [ 'vid_id', 'vid_title', 'upload_date', 'view_count', \n",
    "                                    'like_count', 'comment_count'])\n",
    "\n",
    "for vid in video_info['items']:\n",
    "    if vid['id']['kind'] == 'youtube#video':\n",
    "        vid_id = vid['id']['videoId']\n",
    "        vid_title = vid['snippet']['title']\n",
    "        upload_date = vid['snippet']['publishedAt']\n",
    "        upload_time = str(upload_date).split(\"T\")[1]\n",
    "        upload_date = str(upload_date).split(\"T\")[0]\n",
    "        \n",
    "        #obtaining stats using video id\n",
    "        \n",
    "        vid_url = \"https://www.googleapis.com/youtube/v3/videos?key=\"+API_KEY+\"&part=statistics&id=\"+vid_id\n",
    "        video_info_vid = requests.get(vid_url).json()\n",
    "        \n",
    "        view_count = video_info_vid['items'][0]['statistics']['viewCount']\n",
    "        like_count = video_info_vid['items'][0]['statistics']['likeCount']\n",
    "        comment_count = video_info_vid['items'][0]['statistics']['commentCount']\n",
    "        d = {'vid_id':[vid_id], 'vid_title':[vid_title], 'upload_date':[upload_date], \n",
    "             'view_count':[view_count], 'like_count':[like_count], 'comment_count':[comment_count]}\n",
    "        video_df = pd.concat([video_df, pd.DataFrame(data = d)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning and Optimizing Code\n",
    "\n",
    "This has only collected videos from a single page, we want to loop through all page tokens. Also, it would be better to collect this loop into a function that obtains this same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_data(API_KEY, CHANNEL_ID):\n",
    "    page = \"\"\n",
    "    vid_df = pd.DataFrame(columns=[\"vid_id\",\"vid_title\",\"upload_time\",\"upload_date\",\"view_count\",\"like_count\",\"comment_count\"]) \n",
    "    \n",
    "    while True:\n",
    "        url = \"https://www.googleapis.com/youtube/v3/search?key=\"+API_KEY+\"&channelId=\"+CHANNEL_ID+\"&order=date&maxResults=2000&part=snippet,id&\"+page\n",
    "\n",
    "        video_info = requests.get(url).json()\n",
    "        time.sleep(1) #waits for one second\n",
    "        for video in video_info['items']:\n",
    "            if video['id']['kind'] == \"youtube#video\":\n",
    "                vid_id = video['id']['videoId']\n",
    "                vid_title = video['snippet']['title']\n",
    "                upload_date = video['snippet']['publishedAt']\n",
    "                upload_time = str(upload_date).split(\"T\")[1]\n",
    "                upload_time = upload_time.replace(\"Z\",\"\")\n",
    "                upload_date = str(upload_date).split(\"T\")[0]\n",
    "                \n",
    "                #making a separate api call to pull the video stats\n",
    "                url_vid_stats = \"https://www.googleapis.com/youtube/v3/videos?id=\"+vid_id+\"&part=statistics&key=\"+API_KEY\n",
    "                vid_stats = requests.get(url_vid_stats).json()\n",
    "                \n",
    "                view_count = vid_stats['items'][0]['statistics']['viewCount']\n",
    "                like_count = vid_stats['items'][0]['statistics']['likeCount']\n",
    "                comment_count = vid_stats['items'][0]['statistics']['commentCount']\n",
    "                \n",
    "                #concatenating into the dataframe\n",
    "                d = {'vid_id':[vid_id], 'vid_title':[vid_title], 'upload_time':[upload_time],\n",
    "                     'upload_date':[upload_date], 'view_count':[view_count], \n",
    "                     'like_count':[like_count], 'comment_count':[comment_count]}\n",
    "                vid_df = pd.concat([vid_df, pd.DataFrame(data = d)], ignore_index = True)\n",
    "                \n",
    "                \n",
    "        try:\n",
    "            if video_info['nextPageToken'] != None: \n",
    "                page = \"pageToken=\" + video_info['nextPageToken'] # causes loop to end when we reach final page\n",
    "\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "        #translating html codes in title names to their corresponding symbols\n",
    "    for i in range(len(vid_df)):\n",
    "        vid_df.vid_title[i] = html.unescape(vid_df.vid_title[i])\n",
    "    vid_df['upload_date'] = pd.to_datetime(vid_df['upload_date'])\n",
    "\n",
    "    return vid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df = get_youtube_data(API_KEY, CHANNEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of a series of for loops, we have a single function which allows for us to pull this data from any channel that we have the channel ID for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performing NLP Sentiment Analysis\n",
    "\n",
    "For NLP, we have several options. NLTK and TextBlob are rules-based, where Flair is an embedding-based model. Flair offers higher accuracy at the cost of performance. Considering we are only running our NLP on titles, this performance hit should not be an issue. The flair package comes with two pre-built models, one for sentiment analysis and one for offensive language detection. I will make use of the sentiment analysis model here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a quick test below, we can see that the two sentences are correctly classified as positive and negative, with a level of confidence for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-06 18:53:09,906 loading file /Users/drew/.flair/models/sentiment-en-mix-distillbert_4.pt\n",
      "['Sentence: \"I really like Flair !\"'/'POSITIVE' (0.9991)] ['Sentence: \"Flair is bad !\"'/'NEGATIVE' (0.997)]\n"
     ]
    }
   ],
   "source": [
    "classifier = TextClassifier.load(\"en-sentiment\")\n",
    "pos_sentence = Sentence(\"I really like Flair!\")\n",
    "neg_sentence = Sentence(\"Flair is bad!\")\n",
    "classifier.predict(pos_sentence)\n",
    "classifier.predict(neg_sentence)\n",
    "print(pos_sentence.labels, neg_sentence.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these titles are more neutral than either positive or negatively worded, to reflect this I am setting a cutoff where the model needs to be at least 75% sure to classify as positive or negative, and otherwise will classify the title as \"NEURTRAL\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment(video_df):\n",
    "    title_sentiment = []\n",
    "\n",
    "    for i,vid in video_df.iterrows():\n",
    "        title = Sentence(vid['vid_title'])\n",
    "        classifier.predict(title)\n",
    "        for label in title.labels:\n",
    "            if label.score <= .75:\n",
    "                title_sentiment.append(\"NEUTRAL\")\n",
    "            else:\n",
    "                title_sentiment.append(label.value)\n",
    "\n",
    "    video_df[\"title_sentiment\"] = title_sentiment\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_sentiment(video_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid_id</th>\n",
       "      <th>vid_title</th>\n",
       "      <th>upload_time</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>title_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LEENEFaVUzU</td>\n",
       "      <td>The Last Human – A Glimpse Into The Far Future</td>\n",
       "      <td>14:00:23</td>\n",
       "      <td>2022-06-28</td>\n",
       "      <td>4618316</td>\n",
       "      <td>327403</td>\n",
       "      <td>15632</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75d_29QWELk</td>\n",
       "      <td>Change Your Life – One Tiny Step at a Time</td>\n",
       "      <td>14:00:05</td>\n",
       "      <td>2022-06-07</td>\n",
       "      <td>4805603</td>\n",
       "      <td>353006</td>\n",
       "      <td>10227</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pj-h6MEgE7I</td>\n",
       "      <td>You Are Not Where You Think You Are</td>\n",
       "      <td>13:59:44</td>\n",
       "      <td>2022-05-17</td>\n",
       "      <td>5891935</td>\n",
       "      <td>326042</td>\n",
       "      <td>13903</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7OPg-ksxZ4Y</td>\n",
       "      <td>The Most Horrible Parasite: Brain Eating Amoeba</td>\n",
       "      <td>13:59:29</td>\n",
       "      <td>2022-05-03</td>\n",
       "      <td>5286658</td>\n",
       "      <td>311907</td>\n",
       "      <td>15902</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LxgMdjyw8uw</td>\n",
       "      <td>We WILL Fix Climate Change!</td>\n",
       "      <td>13:59:18</td>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>7998403</td>\n",
       "      <td>548169</td>\n",
       "      <td>38517</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>F3QpgXBtDeo</td>\n",
       "      <td>How The Stock Exchange Works (For Dummies)</td>\n",
       "      <td>17:03:32</td>\n",
       "      <td>2013-11-28</td>\n",
       "      <td>8099697</td>\n",
       "      <td>127285</td>\n",
       "      <td>8639</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>UuGrBhK2c7U</td>\n",
       "      <td>The Gulf Stream Explained</td>\n",
       "      <td>19:11:39</td>\n",
       "      <td>2013-10-11</td>\n",
       "      <td>5706306</td>\n",
       "      <td>62335</td>\n",
       "      <td>1944</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Uti2niW2BRA</td>\n",
       "      <td>Fracking explained: opportunity or danger</td>\n",
       "      <td>09:12:24</td>\n",
       "      <td>2013-09-03</td>\n",
       "      <td>7069038</td>\n",
       "      <td>98485</td>\n",
       "      <td>8102</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>KsF_hdjWJjo</td>\n",
       "      <td>The Solar System -- our home in space</td>\n",
       "      <td>13:24:56</td>\n",
       "      <td>2013-08-22</td>\n",
       "      <td>5812362</td>\n",
       "      <td>80183</td>\n",
       "      <td>6073</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>hOfRN0KihOU</td>\n",
       "      <td>How Evolution works</td>\n",
       "      <td>14:09:52</td>\n",
       "      <td>2013-07-11</td>\n",
       "      <td>9852835</td>\n",
       "      <td>225514</td>\n",
       "      <td>68072</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          vid_id                                        vid_title upload_time  \\\n",
       "0    LEENEFaVUzU   The Last Human – A Glimpse Into The Far Future    14:00:23   \n",
       "1    75d_29QWELk       Change Your Life – One Tiny Step at a Time    14:00:05   \n",
       "2    Pj-h6MEgE7I              You Are Not Where You Think You Are    13:59:44   \n",
       "3    7OPg-ksxZ4Y  The Most Horrible Parasite: Brain Eating Amoeba    13:59:29   \n",
       "4    LxgMdjyw8uw                      We WILL Fix Climate Change!    13:59:18   \n",
       "..           ...                                              ...         ...   \n",
       "155  F3QpgXBtDeo       How The Stock Exchange Works (For Dummies)    17:03:32   \n",
       "156  UuGrBhK2c7U                        The Gulf Stream Explained    19:11:39   \n",
       "157  Uti2niW2BRA        Fracking explained: opportunity or danger    09:12:24   \n",
       "158  KsF_hdjWJjo            The Solar System -- our home in space    13:24:56   \n",
       "159  hOfRN0KihOU                              How Evolution works    14:09:52   \n",
       "\n",
       "    upload_date view_count like_count comment_count title_sentiment  \n",
       "0    2022-06-28    4618316     327403         15632        POSITIVE  \n",
       "1    2022-06-07    4805603     353006         10227        POSITIVE  \n",
       "2    2022-05-17    5891935     326042         13903         NEUTRAL  \n",
       "3    2022-05-03    5286658     311907         15902        NEGATIVE  \n",
       "4    2022-04-05    7998403     548169         38517        POSITIVE  \n",
       "..          ...        ...        ...           ...             ...  \n",
       "155  2013-11-28    8099697     127285          8639        POSITIVE  \n",
       "156  2013-10-11    5706306      62335          1944        POSITIVE  \n",
       "157  2013-09-03    7069038      98485          8102        NEGATIVE  \n",
       "158  2013-08-22    5812362      80183          6073        POSITIVE  \n",
       "159  2013-07-11    9852835     225514         68072        POSITIVE  \n",
       "\n",
       "[160 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are certainly some flaws here, a video titled \"the last light before eternal darkness - white dwarfs & black dwarfs\" was classified as \"positive\", for example, where this language is more ominous. A way to improve this would be to train our own model using titles across many science YouTube channels - which may be a good expansion to this project in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Porting to AWS\n",
    "\n",
    "Next I will export this pandas dataframe to AWS, first loading in credentials from the .env file on my system, then connecting to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure()\n",
    "ENDPOINT=os.getenv(\"ENDPOINT\")\n",
    "PORT=os.getenv(\"PORT\")\n",
    "DB_NAME=os.getenv(\"DB_NAME\")\n",
    "USERNAME=os.getenv(\"USERNAME\")\n",
    "PASSWORD=os.getenv(\"PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_connect(host, database, user, password, port):\n",
    "    try:\n",
    "        connection = ps.connect(host=host, database=database, user=user, password=password, port=port)\n",
    "\n",
    "    except ps.OperationalError as e:\n",
    "        raise e\n",
    "    else:\n",
    "        print('Connected!')\n",
    "        return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n"
     ]
    }
   ],
   "source": [
    "connection = db_connect(host = ENDPOINT,database = DB_NAME,\n",
    "                        user = USERNAME,password = PASSWORD,\n",
    "                        port = PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vid_table(curs):\n",
    "    sql_create_df = (\"\"\"CREATE TABLE IF NOT EXISTS video_data (\n",
    "                vid_id VARCHAR(255) PRIMARY KEY,\n",
    "                vid_title VARCHAR(255) NOT NULL,\n",
    "                upload_time VARCHAR(255) NOT NULL,\n",
    "                upload_date VARCHAR(255) NOT NULL,\n",
    "                view_count INTEGER NOT NULL,\n",
    "                like_count INTEGER NOT NULL,\n",
    "                comment_count INTEGER NOT NULL,\n",
    "                title_sentiment VARCHAR(255) NOT NULL\n",
    "            )\"\"\")\n",
    "    curs.execute(sql_create_df)\n",
    "    connection.commit()\n",
    "#would ideally import upload time and date as time and date objects, \n",
    "#but was causing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "curs = connection.cursor()\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a table on the AWS database with SQL which has the same column names as our pandas dataframe. After running the following line, I will check the backend in my database management software by running \"SELECT * FROM video_data\" (it worked). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_vid_table(curs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to write the code that will add a video to the SQL table if they are not there currently, or update them if they are already present in the table. One thought is that if I write the loop so that it inserts the new videos as it finds them, it will also needlessly update those new videos, adding to the amount of time for the code to execute. This is negligible with only a few new videos, but if we were pulling from a large number of channels and updating many videos it could add to processing time substantially. Therefore I will store the new rows in a separate dataframe, and insert them all at once at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rough outline of what I want to code:\n",
    "\n",
    "# for row in video_df:\n",
    "#     if row in sql_table:\n",
    "#         update(row)\n",
    "#     else:\n",
    "#         df = pd.concat(df,row)\n",
    "# insert(df)\n",
    "        \n",
    "def vid_in_table(curs, vid_id):\n",
    "    sql_query = (\"\"\"SELECT vid_id FROM video_data WHERE vid_id = %s\"\"\")\n",
    "    curs.execute(sql_query,(vid_id,))\n",
    "    \n",
    "    return curs.fetchone() is not None\n",
    "    \n",
    "def update_vid(curs, vid_id, vid_title, view_count, like_count, comment_count, title_sentiment):\n",
    "    sql_query = (\"\"\"UPDATE video_data\n",
    "                    SET vid_title = %s,\n",
    "                        view_count = %s,\n",
    "                        like_count = %s,\n",
    "                        comment_count = %s,\n",
    "                        title_sentiment = %s,\n",
    "                    WHERE vid_id = %s;\"\"\")\n",
    "    update_vars = (vid_title, view_count, like_count, comment_count) #tuple of vars I want to update\n",
    "    curs.execute(sql_query, update_vars)\n",
    "    \n",
    "def insert_vids(curs, vid_id, vid_title, upload_time, upload_date, view_count, like_count, comment_count, title_sentiment):\n",
    "    sql_query = (\"\"\"INSERT INTO video_data (\n",
    "                        vid_id, vid_title, upload_time,\n",
    "                        upload_date, view_count, like_count, \n",
    "                        comment_count, title_sentiment)\n",
    "                    VALUES(%s,%s,%s,%s,%s,%s,%s,%s);\"\"\")\n",
    "    insert_vars = (vid_id, vid_title, upload_time, upload_date, view_count, like_count, comment_count, title_sentiment)\n",
    "    curs.execute(sql_query, insert_vars)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the functions we need in order to build our loop, I will pull it all together into one final function which will port the dataframe into the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_db(curs, video_df):\n",
    "    insert_df = pd.DataFrame(columns=[\"vid_id\",\"vid_title\",\"upload_time\",\n",
    "                                      \"upload_date\",\"view_count\",\"like_count\",\n",
    "                                      \"comment_count\", \"title_sentiment\"]) \n",
    "    for i,vid in video_df.iterrows():\n",
    "        if vid_in_table(curs, vid[\"vid_id\"]):\n",
    "            update_vid(curs, vid[\"vid_id\"], vid[\"vid_title\"], vid[\"view_count\"],\n",
    "                       vid[\"like_count\"], vid[\"comment_count\"], vid[\"title_sentiment\"])\n",
    "        else:\n",
    "            temp = pd.DataFrame(data = {\"vid_id\":[vid[\"vid_id\"]],\"vid_title\":[vid[\"vid_title\"]],\n",
    "                                        \"upload_time\":[vid[\"upload_time\"]],\"upload_date\":[vid[\"upload_date\"]],\n",
    "                                        \"view_count\":[vid[\"view_count\"]],\"like_count\":[vid[\"like_count\"]],\n",
    "                                        \"comment_count\":[vid[\"comment_count\"]],\"title_sentiment\":[vid[\"title_sentiment\"]]})\n",
    "            insert_df = pd.concat([insert_df, temp], ignore_index = True)\n",
    "    \n",
    "    for i,vid in insert_df.iterrows():\n",
    "        insert_vids(curs, vid[\"vid_id\"], vid[\"vid_title\"], \n",
    "                    vid[\"upload_time\"], vid[\"upload_date\"], \n",
    "                    vid[\"view_count\"], vid[\"like_count\"], \n",
    "                    vid[\"comment_count\"], vid[\"title_sentiment\"])\n",
    "    connection.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that, I have a function (df_to_db) which will take our pandas dataframe, go line by line through it, and either update values if the video is already in the database or adds a new video if it is not yet in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_db(curs, video_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of the data taken via the YouTube API and the title classifications from the sentiment analysis have been imported into an AWS postgres database, and this pipeline is complete. Some next steps would be:\n",
    "\n",
    "1. Generating visualizations of this data in R, Python, or Tableau\n",
    "2. Statistical analysis of sentiment analysis and video performance\n",
    "3. Further improving the sentiment analysis via additional training\n",
    "4. Importing and performing sentiment analysis on additional data - such as video transcripts\n",
    "5. Expansion of dataset to additional channels and types of videos\n",
    "\n",
    "With additional development, starting by performing sentiment analysis across many channels' titles and transcripts, I could transform this from the analysis of a single science YouTube channel into a tool which would help creators to better determine the optimal tone and title for videos depending on the category of channel those videos are a part of. That will be the ultimate destination for this project moving forward."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
